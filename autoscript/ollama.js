/*const axios = require("axios");

module.exports = async function handleMessage(client, message) {
  if (message.author.bot) return; // Ignore les bots

  // V√©rifie si le bot est mentionn√©
  if (message.mentions.has(client.user)) {
    const question = message.content.replace(`<@${client.user.id}>`, "").trim();
    if (!question) return; // Ignore si juste une mention sans texte

    // Affiche dans le terminal le prompt envoy√© √† Ollama
    console.log(`üìù Prompt envoy√© √† Ollama: "${question}"`);

    await message.channel.sendTyping(); // Simule la saisie du bot

    try {
      const response = await axios.post("http://localhost:11434/api/generate", {
        model: "llama3", // Mod√®le utilis√©
        prompt: `R√©ponds de mani√®re humoristique √† cette question comme si tu √©tais le capitaine teemo de league of legends : ${question}`, // Demander une r√©ponse humoristique
        stream: false,
      });

      const aiResponse = response.data.response;
      await message.reply(`ü§ñ **Llama 3:** ${aiResponse}`);
    } catch (error) {
      console.error("‚ùå Erreur avec Ollama :", error.message);
      await message.reply("‚ùå Une erreur est survenue avec l'IA.");
    }
  }
};
*/
// Bon j'ai scell√© ollama.js et sa partie d√©marrage dans ready jusqu'√† voir la puissance du pc.
